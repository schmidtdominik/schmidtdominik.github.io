<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Information Theory - Dominik Schmidt</title>
  <meta name="author" content="Dominik Schmidt">
  <meta name="theme-color" content="#52D681"/>

  <link rel=preload href="../css/BIG JOHN.otf" as="font" crossorigin>

  <meta property="og:title" content="Information Theory"/>
  <meta property='og:type' content='article'/>
  <meta property="og:image" content="assets/cover.png"/>
  <meta property="og:url" content="http://dominikschmidt.xyz/information-theory-notes"/>
  <!--<meta property="og:description" content="These are my lecture notes on Prof. Raymond W. Yeung's excellent course on information theory. The course is based on Prog. Yeung's book "Information Theory and
Network Coding" and these notes are organized around that.<br>
<br>

Information theory is a major research field in communication theory and applied probability. This is not meant as a comprehensive guide but rather a loose collection of definitions and short summaries."/>-->

  <link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
  <script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
  <script>
  window.addEventListener("load", function(){
  window.cookieconsent.initialise({
    "palette": {
      "popup": {
        "background": "#edeff5",
        "text": "#838391"
      },
      "button": {
        "background": "#4b81e8"
      }
    },
    "position": "bottom-left",
    "content": {
      "href": "http://dominikschmidt.xyz/policy"
    }
  })});
  </script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123900161-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-123900161-1');
  </script>

  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-4694920180315448",
            enable_page_level_ads: true
       });
  </script>

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <script defer src="https://use.fontawesome.com/releases/v5.12.0/js/all.js"></script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
           CommonHTML: {
             scale: (MathJax.Hub.Browser.isChrome ? 100 : 100)
           }
         });


  </script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>


  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="../css/normalize.css">
  <link rel="stylesheet" href="../css/skeleton.css">
  <link rel="stylesheet" href="../css/main.css">
  <link rel="stylesheet" href="../css/article.css">

  <link rel="stylesheet" href="../highlight/styles/atom-one-light.css"><!--foundation/hopscotch-->
  <script src="../highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="../images/favicon.png">

  <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script><script type="text/javascript">window.dojoRequire(["mojo/signup-forms/Loader"], function(L) { L.start({"baseUrl":"mc.us19.list-manage.com","uuid":"728374c3215a28cdf09e2b11c","lid":"eeb0dbccb1","uniqueMethods":true}) })</script>
</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <div class="row">
      <div style="margin-top: 8%">
          <h1>
            <a href="../index.html">
              <span id="title">
                Dominik Schmidt
              </span>
            </a>
            <!--<a href="index.html">
              <span id="by-me">

              </span>
            </a>-->
          </h1>
        <hr>
        <nav>
          <span style="display: inline-block;" ><a class="navitem" href="../index.html"><i class="far fa-newspaper"></i>&nbsp;Reads</a></span>
          <span style="display: inline-block;" ><a class="navitem" href="../datasets.html"><i class="fas fa-sitemap"></i>&nbsp;Datasets</a></span>
          <span style="display: inline-block;" ><a class="navitem" href="../random.html"><i class="fas fa-robot"></i>&nbsp;Random Stuff</a></span>
          <span style="display: inline-block;" ><a class="navitem" href="../about.html"><i class="fas fa-map-marker-alt"></i>&nbsp;About me</a></span>

          <span class="navwrapper" style="float: right"><a class="navitem" style="margin-right: 1.6rem;" href="https://github.com/schmidtdominik"><i class="fab fa-github"></i>&nbsp;</a></span>
          <!--<span class="navwrapper" style="float: right"><a class="navitem" style="margin-right: 1.6rem;" href="https://www.reddit.com/user/dominik_schmidt"><i class="fab fa-reddit"></i></i>&nbsp;</a></span>-->
        </nav>
      </div>
    </div>
  </div>

  <div class="container article">
      <div id='header-box'>


        <!--START OF HEADING-->
        <div style="display:flex;justify-content:center;align-items:center;">
          <img src="assets/cover.png" id="header-img" alt="">
        </div>


        <div id="header-text">
          <h1 class="article-title">
            Information Theory
          </h1>
          <time datetime="2020-02-16 00:00:00">February 16, 2020 ⧗ 24 minute read</time><br>
        </div>
      </div>
      <!--END OF HEADING-->

      <span class="article-text">
        <div id="index"><a href=#1.-the-science-of-information> → 1. The Science of Information</a><br><a href=#shannon's-seminal-paper-1948>&emsp; → Shannon's Seminal Paper (1948)</a><br><a href=#2.-information-measures> → 2. Information Measures</a><br><a href=#2.1-independence-and-markov-chain>&emsp; → 2.1 Independence and Markov Chain</a><br><a href=#2.2-shannon's-information-measures>&emsp; → 2.2 Shannon's Information Measures</a><br><a href=#2.3-continuity-of-shannon's-information-measures>&emsp; → 2.3 Continuity of Shannon's Information Measures</a><br><a href=#2.4-chain-rules>&emsp; → 2.4 Chain Rules</a><br><a href=#2.5-informational-divergence>&emsp; → 2.5 Informational Divergence</a><br><a href=#2.6-the-basic-inequalities>&emsp; → 2.6 The Basic Inequalities</a><br><a href=#2.7-some-useful-information-inequalities>&emsp; → 2.7 Some Useful Information Inequalities</a><br><a href=#2.8-fano's-inequality>&emsp; → 2.8 Fano's Inequality</a><br><a href=#2.10-entropy-rate-of-a-stationary-source>&emsp; → 2.10 Entropy Rate of a Stationary Source</a><br><a href=#3.-the-i-measure> → 3. The I-Measure</a><br><a href=#3.1-preliminaries>&emsp; → 3.1 Preliminaries</a><br><a href=#3.3-construction-of-the-i-measure-\mu^\ast>&emsp; → 3.3 Construction of the I-Measure \(\mu^\ast\)</a><br><a href=#3.4-\mu^\ast-can-be-negative>&emsp; → 3.4 \(\mu^\ast\) can be Negative</a><br><a href=#3.5-information-diagrams>&emsp; → 3.5 Information Diagrams</a><br><a href=#4.-zero-error-data-compression> → 4. Zero-Error Data Compression</a><br><a href=#4.1-the-entropy-bound>&emsp; → 4.1 The Entropy Bound</a><br><a href=#4.2-prefix-codes>&emsp; → 4.2 Prefix Codes</a></div><br><hr>These are my lecture notes on Prof. Raymond W. Yeung's excellent course on information theory. The course is based on Prog. Yeung's book "Information Theory and
Network Coding" and these notes are organized around that.<br>
<br>

Information theory is a major research field in communication theory and applied probability. This is not meant as a comprehensive guide but rather a loose collection of definitions and short summaries.<br>
<br>
<h2 id="1.-the-science-of-information"> 1. The Science of Information</h2><br>

<div class="image-center-wrapper"><img src="assets/txrx_model.png" style="width: 80%;" class="inline-image image-center    "></div>
Two key concepts:<br>
<br>

<ul>
<li> Information is uncertainty: modeled as random variables</li>
<li> information is transmitted digitally: transmission is based on ones and zeros with no reference to what they represent</li>
</ul><br>
<br>
<h3 id="shannon's-seminal-paper-1948"> Shannon's Seminal Paper (1948)</h3>
<ul>
<li> The <strong>source coding theorem</strong> defines <strong>entropy</strong> as the fundamental measure of information and establishes a fundamental limit for data compression (there always exists a minimal compressed-file size, no matter how smart the compression). This is the theoretical basis for lossless data compression.</li>
<li> The <strong>channel coding theorem</strong> establishes a fundamental rate limit for reliable communication through a noisy channel. There always exists a maximum rate, called the <strong>channel capacity </strong>(which is generally strictly positive), of how much data can be reliably transmitted through a a channel.</li>
</ul><br>
<br>
<h2 id="2.-information-measures"> 2. Information Measures</h2>
<ul>
<li> \(X\) is a <strong>discrete random variable</strong> taking values in \(\mathcal{X}\)</li>
<li> \({p_X(x)}\) is the probability distribution for \(X\)</li>
<li> \(\mathcal{S}_X=\{x\in\mathcal{X}:{p_X(x)>0}\}\) is the <strong>support</strong> of X (set of all outcomes \(x\) such that the probability is non-zero)</li>
<li> if \(\mathcal{S}_X=\mathcal{X}\), \(p\) is called <strong>strictly positive</strong></li>
</ul><br>
<br>
<h3 id="2.1-independence-and-markov-chain"> 2.1 Independence and Markov Chain</h3>
Random variables \(X\) and \(Y\) are called <strong>independent</strong>, denoted by \(X\perp Y\) if:
$$
\forall (x,y) \in \mathcal{X\times Y}\colon\, p(x, y)=p(x)p(y)
$$<br>

For \(n\ge3\), random variables \(X_1, X_2,...X_n\) are <strong>mutually independent</strong> if:
$$
\forall (x_1, x_2,...,x_n)\colon\, p(x_1, x_2,...,x_n)=p(x_1)p(x_2)\cdots p(x_n)
$$<br>

For \(n\ge3\), random variables \(X_1, X_2,...X_n\) are <strong>pairwise independent</strong> if all \(X_i\) and \(X_j\) are independent.<br>

Mutual independence \(\implies\) pairwise independence<br>
<br>

For random variables \(X\), \(Y\) and \(Z\), \(X\) is independent of \(Z\) conditioning on \(Y\), denoted by \((X\perp Z)\mid Y\) (<strong>conditional independence</strong>)
$$
p(x, y, z) = \Bigg\{
		\begin{array}{lr}
        \frac{p(x,y)p(y,z)}{p(y)}\,(\ast), & \text{if } p(y)>0\\
        0, & \text{otherwise}
        \end{array}
$$<br>

if \(p(y)>0\) then
$$
p(x,y,z)=(\ast)\,\frac{p(x,y)p(y,z)}{p(y)}=p(x,y)p(z|y)
$$<br>

<img src="assets/cond_indep.svg" style="width: 40%;" class="inline-image inline-image-right    "> "In other words, \(A\) and \(B\) are conditionally independent given \(C\) if and only if, given knowledge that \(C\) occurs, knowledge of whether \(A\) occurs provides no information on the likelihood of \(B\) occurring, and knowledge of whether \(B\) occurs provides no information on the likelihood of \(A\) occurring." <sup><a id="ref1ref" href="#ref1">[1]</a></sup> In the picture on the right \(R\) and \(B\) are conditionally independent given \(Y\) but not given \(\overline Y\)<br>
<br>

For random variables \(X\), \(Y\) and \(Z\)
$$
\forall (x, y, z),\, p(y)>0\colon\, [(X\perp Z)\mid Y \iff p(x,y,z) = a(x,y)b(y,z)]
$$
Meaning <strong>\(p(x,y,z)\) can be factorized</strong> as the given term, where \(a\) is a function that depends only on \(x, y\) and \(b\) is a function that depends only on \(y, z\).<br>
<br>

For random variables \(X_1, X_2,\ldots,X_n\) where \(n\ge 3\), \(X_1→X_2→\cdots→X_n\) forms a <strong>Markov chain</strong> if
$$
p(x_1, x_2,\ldots,x_n) = \\\Bigg\{\begin{array}{lr}
        p(x_1, x_2)p(x_3|x_2)\cdots p(x_n|x_{n-1}) & \text{if } p(x_2), p(x_3),\ldots,p(x_{n-1})>0\\
        0, & \text{otherwise}
        \end{array}
$$<br>

or equivalently when \(p(x_1, \ldots, x_n)p(x_2)\cdots p(x_{n-1}) = p(x_1, x_2)p(x_2, x_3)\cdots p(x_{n-1}, x_n)\).<br>

Note that this definition allows for non-stationary transition probabilities, i.e. the probability distribution can still change over time, see stationarity below.<br>
<br>

<ul>
<li>\(X_1→X_2→X_3 \iff (X_1\perp X_3)\mid X_2\)</li>
<li>\(X_1→X_2→\cdots→X_n\) forms a Markov chain \(\iff X_n→X_{n-1}→\cdots X_1\) forms a Markov chain</li>
<li>\(X_1→X_2→\cdots→X_n\) forms a Markov chain if and only if
$$
X_1→X_2→X_3\\
(X_1,X_2)→X_3→X_4
\\\vdots\\
(X_1, X_2, \ldots, X_{n-2})→X_{n-1}→X_n
$$
form Markov chains.</li><br>

<li>\(X_1→X_2→\cdots→X_n\) forms a Markov chain if and only if
$$
p(x_1, x_2,\ldots,x_n) =f_1(x_1, x_2)f_2(x_2,x_3)\cdots f_{n-1}(x_{n-1},x_2)
$$
This is a generalization of Proposition 2.5</li>
</ul><br>
<br>

<strong>Markov subchains:</strong> Let \(\mathcal{N}_n=\{1,2,\ldots n\}\) and let \(X_1→X_2→\cdots→X_n\) form a Markov chain. For any subset \(\alpha\) of \(\mathcal{N}_n\) denote \((X_i, i \in \alpha)\) (a collection of random Variables) by \(X_\alpha\). Then for any disjoint subsets \(\alpha_1, \alpha_2, \ldots \alpha_m\) of \(\mathcal{N}_n\) such that
$$
k_1 < k_2 < \cdots < k_m
$$
for all \(k_j \in \alpha_j, \,j=1,2,\ldots, m\),
$$
X_{\alpha_1}→X_{\alpha_2}→\cdots→X_{\alpha_n}
$$
forms a Markov chain. Thats is, a subchain of \(X_1→X_2→\cdots→X_n\) is also a Markov chain.<br>
<br>
<br>
<h3 id="2.2-shannon's-information-measures"> 2.2 Shannon's Information Measures</h3>
Shannon introduced these basic measures of information:<br>
<br>

<ul>
<li><strong>Entropy</strong> \(H(X) = -\sum_xp(x)\log_\alpha p(x) = -E[\log p(X)]\)<br>

Measures the uncertainty of a discrete random variable. The unit for entropy is <strong>bit</strong> if \(\alpha=2\), <strong>nat</strong> if \(\alpha=e\) and D-it if \(\alpha = D\). (A bit in information theory is different from a bit in computer science)</li>
<li><strong>Joint Entropy</strong> \(H(X, Y) = -\sum_{x, y} p(x, y) \log p(x,y) = -E[\log p(X,Y)]\)<br>

Measures the uncertainty of two joint discrete random variables.
</li>
<li><strong>Conditional Entropy</strong> \(H(Y|X) = -\sum_{x, y}  p(x, y) \log p(y|x) = -E[\log p(Y|X)]\)<br>

Measures the uncertainty of a discrete random variable Y, given X.
</li>
<li><strong>Mutual Information</strong> \(I(X; Y) = \sum_{x, y} p(x, y)\log \frac{p(x,y)}{p(x)p(y)}=E[\log \frac{p(X,Y)}{p(X)p(Y)}]\)<br>

"Quantifies the "amount of information" [..] obtained about one random variable through observing the other random variable."<sup><a id="ref2ref" href="#ref2">[2]</a></sup>
</li>
<li><strong>Conditional Mutual Information</strong> \(I(X; Y|Z) = \sum_{x,y,z} \log \frac{p(x, y|z)}{p(x|z)p(y|z)} = E[\log \frac{p(X, Y|Z)}{p(X|Z)p(Y|Z)}]\)<br>

The mutual information of X and Y, given Z.
</li>
</ul><br>

All of the above information measures can be expressed in terms of conditional mutual information.<br>

<div class="image-center-wrapper"><img src="assets/information-measures.png" style="width: 60%;" class="inline-image image-center    "></div>
The following equalities hold:<br>
<br>

<ul>
    <li>\(H(X, Y) = H(Y, X)\) and \(I(X; Y) = I(Y; X)\) and \(I(X; Y | Z) = I(Y; X | Z)\)(symmetry)</li>
    <li>\(H(X, Y) = H(X) + H(Y|X)\) (revealing X and Y at the same time or one after another yields the same amount of information)</li>
    <li>\(I(X; X) = H(X)\)</li>
    <li>\(I(X; Y) = H(X) - H(X|Y)\)</li>
    <li>\(I(X; Y) = H(X) + H(Y) - H(X, Y)\) (→ inclusion-exclusion)</li>
    <li>\(I(X; X|Z) = H(X|Z)\)</li>
</ul><br>
<br>
<h3 id="2.3-continuity-of-shannon's-information-measures"> 2.3 Continuity of Shannon's Information Measures</h3>
All of the information measures described above are continuous for fixed finite alphabets with respect to convergence in variational distance (\(\mathcal{L}_1\) distance):
$$
V(p, q) = \sum_{x\in\mathcal{X}} |p(x) - q(x)|
$$<br>
<h3 id="2.4-chain-rules"> 2.4 Chain Rules</h3>
<ul>
<li><strong>Chain Rule for Entropy</strong> \(H(X_1, X_2, \ldots, X_n) = \sum_{i=1}^n H(X_i|X_1, \ldots, X_{i-1})\)</li>
<li><strong>Chain Rule for Conditional Entropy</strong></li>
<li><strong>Chain Rule for Mutual Information</strong> \(I(X_1, X_2, \ldots, X_n; Y) = \sum_{i=1}^n I(X_i;Y|X_1, \ldots, X_{i-1})\)</li>
<li><strong>Chain Rule for Conditional Mutual Information</strong></li>
</ul><br>
<br>
<h3 id="2.5-informational-divergence"> 2.5 Informational Divergence</h3>
The <strong>informational divergence</strong> (Kullback-Leibler distance/relative entropy) between two probability distributions \(p\) and \(q\) on a common alphabet \(\mathcal{X}\) is defined as:
$$
D(p\|q)=\sum_xp(x)\log\frac{p(x)}{q(x)} = E_p\log\frac{p(X)}{q(X)} \underset{\text{"Div. Ineq."}}{\ge} 0
$$
(where \(E_p\) denotes expectation with respect to \(p\), note that \(D\) is not symmetrical)<br>
<br>

<strong>Fundamental Inequality:</strong> \(\ln a \le a-1\) for \(a > 0\)<br>

<strong>Log-Sum Inequality:</strong>
$$
\sum_i a_i \log \frac{a_i}{b_i} \ge \bigg(\sum_i a_i\bigg) \log \frac{\sum_i a_i}{\sum_i b_i}
$$
Log-Sum Inequality \(\iff\) Divergence Inequality<br>
<br>

<strong>Pinsker's Inequality:</strong> \(D(p\|q) \ge \frac{1}{2\ln 2} V^2(p, q)\)<br>

In particular, convergence in informational divergence \(\implies\) convergence in variational distance<br>
<br>
<h3 id="2.6-the-basic-inequalities"> 2.6 The Basic Inequalities</h3>
<ul>
<li>\(I(X; Y|Z) \ge 0\) (follows from the divergence inequality)</li>
<li>\(H(X)=0 \iff X\) is deterministic</li>
<li>\(H(Y|X) = 0 \iff Y\) is a function of \(X\)</li>
<li>\(H(Y) \le H(X) \iff Y\) is a function of \(X\) (deterministically processing a random variable cannot increase entropy)s</li>
<li>\(I(X; Y) = 0 \iff X\) and \(Y\) are independent</li>
</ul><br>
<br>
<h3 id="2.7-some-useful-information-inequalities"> 2.7 Some Useful Information Inequalities</h3>
<ul>
    <li><strong>Conditioning does not increase entropy:</strong> \(H(Y|X) \le H(Y)\) and \(H(Y|X, Z) \le H(Y|Z)\) (equality iff \(X\) and \(Y\) are independent)
    Note however that conditioning can increase mutual information (page 60).
    </li>
    <li><strong>Independence bound for entropy:</strong> \(H(X_1, X_2, \ldots, X_n) \le \sum_{i=1}^n H(X_i)\) (equality iff all \(X_i\) are mutually independent)</li>
    <li>\(I(X; Y, Z) \le I(X; Y)\) (equality iff \(X → Y → Z\) forms a MC)</li>
    <li><strong>Closer variables on the MC have higher mutual information:</strong> If \(X→Y→Z\) then \(I(X;Z) \le I(X;Y)\) and  \(I(X;Z) \le I(Y;Z)\)</li>
    <li><strong>Closer pairs of variables on a MC have lower conditional entropy:</strong> \(X→Y→Z \implies H(X|Z) \ge H(X|Y)\)</li>
    <li><strong>Data processing inequality</strong> If \(U→X→Y→V\) forms a MC then \(I(U; V) \le I(X; Y)\) ("post-processing cannot increase information"<sup><a id="ref3ref" href="#ref3">[3]</a></sup>)</li>
</ul><br>
<h3 id="2.8-fano's-inequality"> 2.8 Fano's Inequality</h3>
Theorem: \(H(X) \le \log |\mathcal{X}|\). Therefore if \(\mathcal{X}\) is finite then \(H(X)\) is finite.<br>
<br>

A random variable with an infinite alphabet can have finite or infinite entropy, see \(X\) with \(\Pr\{X=i\}=2^{-i}\) (example 2.45) or example 2.46 in the textbook.</li><br>
<br>

<strong>Fano's Inequality</strong><br>

Let \(X\) and \(\hat X\) be random variables on the alphabet \(\mathcal{X}\). Then
$$
H(X|\hat X) \le h_b(P_e)+P_e \log (|\mathcal{X}|-1)
$$
where \(P_e = \Pr\{X\neq\hat X\}\) and \(h_b\) is the binary entropy function.<br>

Suppose \(\hat X\) is an estimate on \(X\). If the error probability \(P_e\) is small, then \(H(X|\hat X)\) should be small as well.<br>
<br>

For a finite alphabet \(P_e → 0 \implies H(X|\hat X) → 0\)<br>
<br>

Corollary: \(H(X|\hat X) \le 1+P_e \log (|\mathcal{X}|)\)<br>
<br>
<h3 id="2.10-entropy-rate-of-a-stationary-source"> 2.10 Entropy Rate of a Stationary Source</h3>
A discrete-time information source can be modeled as a <strong>discrete-time random process</strong> \(P=\{X_k, k \ge 1\}\). \(P\) is an infinite collection of random variables indexed by the set of positive integers. The index \(k\) is referred to as the <strong>"time"</strong> index. Random variables \(X_k\) are called <strong>letters</strong>. We assume that all \(H(X_k)\) are finite.<br>
<br>

The total entropy of a finite subset of \(\{X_k\}\) (\(H(X_k, k\in A)\) where \(A \subset P\)) is finite because of the independence bound for entropy and our assumption that all \(H(X_k)\) are finite. Apart from special cases the joint entropy of an infinite collection of letters is usually infinite, therefore it is generally not meaningful to discuss the entropy of \(P\). <br>
<br>

<ul>
<li>The <strong>entropy rate</strong> \(H_X\) is defined as \(\lim_{n→\infty} \frac{1}{n}H(X_1, \ldots, X_n)\).<br>

If the limit does not exist, \(H_X\) is not defined.</li>
<li>Define \(H_X^\prime\) as \(\lim_{n→\infty} H(X_n | X_1, X_2, \ldots, X_{n-1})\)<br>

This is the limit of the conditional entropy of the next letter given the past history of the source.
</li>
</ul><br>

An information source \(\{X_k\}\) is called <strong>stationary</strong> if \(X_1, X_2, \ldots, X_m\) and  \(X_{1+l}, X_{2+l}, \ldots, X_{m+l}\) have the same joint distribution for any \(m, l \ge 1\).<br>
<br>

For a sequence \((a_n)\), the <strong>Cesáro mean</strong> is defined as the sequence \((b_n)\), where \(b_n = \frac{1}{n}\sum_{i=1}^n a_i\). If \(a_n → a\) then \(b_n → a\).<br>
<br>

Source stationary \(\implies\) the limit \(H_X^\prime\) exists \(\implies\) the entropy rate \(H_X\) exists and is equal to \(H_X^\prime\). (proof using stationarity and Cesáro mean) Therefore for stationary sources, \(H_X^\prime\) is simply an alternative definition/interpretation of the entropy rate.<br>
<br>
<h2 id="3.-the-i-measure"> 3. The I-Measure</h2>
Shannon's information measures for \(n \ge 2\) random variables have a set-theoretic structure.<br>
<br>

<ul>
	<li>The random variable \(X\) corresponds to the set \(\tilde X\)</li>
    <li>\(H/I\) corresponds to \(\mu^\ast\), where \(\mu^\ast\) is some signed measure (set-additive function)</li>
    <li>\(,\) corresponds to \(\cup\)</li>
    <li>\(;\) corresponds to \(\cap\)</li>
    <li>\(|\) corresponds to \(-\) where \((A-B = A \cap B^C)\)</li>
</ul><br>

Notation<br>
<br>

<ul>
    <li>\(X_G = (X_i, i \in G)\)</li>
    <li>\(\tilde X_G = \cup_{i\in G} \tilde X_i\)</li>
</ul><br>
<h3 id="3.1-preliminaries"> 3.1 Preliminaries</h3>
The <strong>field</strong> \(\mathcal{F}_n\) generated by sets \(\tilde X_1, \tilde X_2, \ldots, \tilde X_n\) is the collection of sets which can be obtained by any sequence of usual set operations on \(\tilde X_1, \tilde X_2, \ldots, \tilde X_n\).<br>
<br>

The <strong>atoms</strong> of \(\mathcal{F}_n\) are sets of the form \(\bigcap_{i=1}^n Y_i\) where \(Y_i\) is either \(\tilde X_i\) or \(\tilde X_1^C\).<br>
<br>

A real function \(\mu\) defined on \(\mathcal{F}_n\) is called a <strong>signed measure</strong> if it is <strong>set-additive:</strong> for <em>disjoint</em> \(A, B\) in \(\mathcal{F}_n\): \(\mu(A \cup B) = \mu(A) + \mu(B)\).<br>
<br>

A signed measure on \(\mathcal{F}_n\) is completely specified by its values on the atoms of \(\mathcal{F}_n\) , \(\{\mu(A), A \in \mathcal{A} \}\), since the values on other sets in \(\mathcal{F}_n\) can be obtained via set-additivity.<br>

(\(\ast\)) A signed measure is furthermore completely specified by it's values on all unions \(\{\mu(B), B \in \mathcal{B} \}\) where \(\mathcal{B} = \{\tilde X_G : G \subset \mathcal{N_n}\}\).<br>
<br>
<h3 id="3.3-construction-of-the-i-measure-$\mu^\ast$"> 3.3 Construction of the I-Measure \(\mu^\ast\)</h3>
<ul>
    <li>Let \(\tilde X_i\) be the set corresponding to the random variable \(X_i\)</li>
    <li>For a fix \(n\) let \(\mathcal{N_n} = \{1, 2, \ldots, n\}\)</li>
    <li>Let the universal set be \(\Omega = \bigcup_{i\in\mathcal{N_n}} \tilde X_i\)</li>
    <li>The atom
        $$
        A_0 = \bigcap_{i\in \mathcal{N_n}} \tilde X_i^C = {\Bigg(\bigcup_{i \in \mathcal{N_n}} \tilde X_i \Bigg)}^C = \Omega^C = \emptyset
        $$
	is called the <strong>empty atom</strong> of \(\mathcal{F_n}\).
    </li>
    <li>\(\mathcal{A}\) is the set of all other atoms of \(\mathcal{F_n}\), called the <strong>non-empty atoms</strong> and \(|A| = 2^n-1\).</li>
</ul><br>

<strong>Definition I-measure:</strong> Define \(\mu^\ast\) by setting 
$$\mu^\ast(\tilde X_G) = H(X_G)$$
for all non-empty \(G\subset \mathcal{N_n}\). This completely defines the measure as by (\(\ast\)). \(\mu^\ast\) is the unique signed measure on \(\mathcal{F_n}\) which is consistent with all information measures.<br>
<br>

To show that \(\mu^\ast\) is consistent with all information measures we only have to show that it is consistent with conditional mutual information, i.e. that \(\mu^\ast(\tilde X_G \cap \tilde X_{G'}-\tilde X_{G''}) = H(X_G; X_{G'}|X_{G''})\). This can be proved using Lemmas 3.7 and 3.8 in the textbook.<br>
<br>

Using the I-measure we can employ set-theoretic tools to manipulate expressions of Shannon's information measures.<br>
<br>
<h3 id="3.4-$\mu^\ast$-can-be-negative"> 3.4 \(\mu^\ast\) can be Negative</h3>
\(\mu^\ast\) is positive for all non-empty atoms that correspond to Shannon's information measures. It can however be negative in other cases such as \(\mu^\ast(\tilde X_1 \cap \tilde X_2 \cap \tilde X_3) = I(X_1;X_2;X_3)\), which is not a Shannon's information measure.<br>
<br>

For two or fewer random variables all three non-empty atoms correspond to Shannon's information measures so here \(\mu^\ast\) is always positive.<br>
<br>
<h3 id="3.5-information-diagrams"> 3.5 Information Diagrams</h3>
Due to the correspondence between information and set theory we can use Venn diagrams to visualize information measures. In \(n\) dimensions, any information diagram for \(n+1\) dimensions can be displayed perfectly.<br>

<div class="image-center-wrapper"><img src="assets/information-diag.png" style="width: 80%;" class="inline-image image-center    "></div>
We can always omit atoms from an information diagram on which \(\mu^\ast\) takes the value \(0\). In particular, this occurs when certain Markov conditions are imposed on the random variables, see the following information diagram for a Markov chain:<br>

<div class="image-center-wrapper"><img src="assets/information-diag-markov.png" style="width: 80%;" class="inline-image image-center    "></div><br>

<ul>
<li>If there is no constraint on \(X_1, X_2, \ldots, X_n\), then \(\mu^\ast\) can take any set of nonnegative values on the nonempty atoms of \(\mathcal{F_n}\).</li>
<li>\(\mu^\ast\) is nonnegative on all atoms of a Markov chain.</li>
<li>If \(\mu^\ast\) is nonnegative, then \(A\subset B \implies \mu^\ast(A) \le \mu^\ast(B)\)</li>
</ul><br>

<strong>Shannon's Perfect Secrecy Theorem</strong><br>

Given plaintext \(X\), ciphertext \(Y\) and key \(Z\).<br>
<br>

<ul>
	<li>Perfect secrecy: \(I(X; Y) = 0\)</li>    
	<li>Decipherability: \(H(X|Y, Z) = 0\)</li>    
</ul><br>

These requirements imply that \(H(Z) \ge H(X)\), i.e. the length of the key is \(\ge\) the length of the plaintext.
<div class="image-center-wrapper"><img src="assets/perfect-secrecy-proof.png" style="width: 90%;" class="inline-image image-center    "></div>
<strong>Imperfect Secrecy Theorem</strong><br>

Given plaintext \(X\), ciphertext \(Y\) and key \(Z\). Decipherability: \(H(X|Y, Z) = 0\)<br>

Generalization of Shannon's perfect secrecy theorem above: \(I(X;Y) \ge H(X) - H(Z)\)<br>

Interpretation: \(I(X; Y)\) measures the leakage of information into the ciphertext<br>
<br>
<h2 id="4.-zero-error-data-compression"> 4. Zero-Error Data Compression</h2><h3 id="4.1-the-entropy-bound"> 4.1 The Entropy Bound</h3>
A D-ary <strong>source code</strong> \(\mathcal{C}\) for a source random variable \(X\) is a function \(\mathcal{C}\colon \mathcal{X}→\mathcal{D^\ast}\), where \(\mathcal{D}^\ast\) is the set of all finite strings from a D-ary alphabet.<br>
<br>

A code \(\mathcal{C}\) is <strong>uniquely decodable</strong> if for any string in \(\mathcal{D^\ast}\) the function can be inverted to get the <em>unique</em> source sequence that generated it.<br>
<br>

<strong>Kraft Inequality</strong>
Let \(\mathcal{C}\) be a D-ary source code and let \(l_1, l_2, \ldots, l_m\) be the lengths of the codewords. If \(\mathcal{C}\) is <em>uniquely decodable</em> then,
$$
\sum_{k=1}^m D^{-l_k}\le 1
$$<br>

Let \(X\) be a source random variable with \(X \sim \{p_1, p_2, \ldots, p_m\}\). Then the <strong>expected code length</strong> \(L\) of a source code \(\mathcal{C}\)  is \(\sum_i p_il_i\).<br>
<br>

For a D-ary <em>uniquely decodable</em> code \(\mathcal{C}\) for a source variable \(X\) we can furthermore establish the <strong>entropy bound</strong> \(H_D(X) \le L\), since each D-ary symbol can carry at most 1 D-it of information.<br>
<br>

The <strong>redundancy</strong> \(R\) of a D-ary uniquely decodable code is \(L-H_D(X) \underset{H \text{-bound}}{\ge} 0\)<br>
<br>
<h3 id="4.2-prefix-codes"> 4.2 Prefix Codes</h3>
A code is <strong>prefix-free</strong> if no codeword is a prefix of another codeword. Such codes are called <strong>prefix codes</strong>. All prefix codes are uniquely decodable.<br>
<br>

A tree representation of a prefix code is called a <strong>code tree</strong>.
<div class="image-center-wrapper"><img src="assets/code-tree.png" style="width: 80%;" class="inline-image image-center    "></div>
<strong>Instantaneous decoding</strong> can be done by tracing the code tree from the root, starting at the beginning of the stream of coded symbols. In this way, the boundaries of the codewords can be discovered. A prefix code is said to be <strong>self-punctuating</strong>.<br>
<br>

There exists a D-ary prefix code with codeword lengths \(l_1, l_2, \ldots, l_m \iff\) the Kraft inequality is satisfied <br>
<br>

A probability distribution is called <strong>D-adic</strong> (or <strong>dyadic</strong> when D=2) when \(p_i = D^{-t_i}\) for all \(i\), where \(t_i\) is an integer.<br>
<br>

There exists a D-ary prefix code which achieves the entropy bound for a distribution \(\{p_i\}\) if and only if  \(\{p_i\}\) is D-adic.
<section id="article_footer"><h2>References</h2><br><ol class="references"><li><span id="ref1">https://en.wikipedia.org/wiki/Conditional_independence</span> <a href="#ref1ref""><i class="references-return fas fa-caret-left"></i></a></li><li><span id="ref2">https://en.wikipedia.org/wiki/Mutual_information</span> <a href="#ref2ref""><i class="references-return fas fa-caret-left"></i></a></li><li><span id="ref3">https://en.wikipedia.org/wiki/Data_processing_inequality</span> <a href="#ref3ref""><i class="references-return fas fa-caret-left"></i></a></li></ol></section>
      </span>

      <!--<div id="disqus_thread"></div>


      <script>
          /**
           *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
           *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
           */

          var disqus_config = function () {
              this.page.url = "https://dominikschmidt.xyz/information-theory-notes";  // Replace PAGE_URL with your page's canonical URL variable
              this.page.identifier = "information-theory-notes"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
          };

          (function() {  // DON'T EDIT BELOW THIS LINE
              var d = document, s = d.createElement('script');

              s.src = 'https://dominikschmidtxyz.disqus.com/embed.js';

              s.setAttribute('data-timestamp', +new Date());
              (d.head || d.body).appendChild(s);
          })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
    </div>-->

  <br><br>

  <!--<footer class="container">
    <span><i class="fas fa-copyright"></i> 2018 Dominik Schmidt</span><br>
  </footer>-

  <br>-->

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>
